{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c413be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9487e7ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=appName, master=local) created by __init__ at <ipython-input-2-d3f460fa7586>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d3f460fa7586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"appName\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# yarn-client ,yarn-cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                     \u001b[1;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    351\u001b[0m                         \u001b[1;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                         \u001b[1;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=appName, master=local) created by __init__ at <ipython-input-2-d3f460fa7586>:2 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\") # yarn-client ,yarn-cluster\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c556f6a-d6b0-411c-899d-0a78a98da70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = SparkConf().setAppName(\"Sagar\").setMaster(\"local\") # yarn-client ,yarn-cluster\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ea6102-5fa8-4a2a-9ad8-aa61a4bbc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d320a47-754e-4d09-a461-3c67e4c71cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print(\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf09d3f1-0618-4945-ab91-cf89ec3d3f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"collect()\n",
    "All the elements in the RDD are returned.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "coll = words.collect()\n",
    "print(\"Elements in RDD -> %s\" % (coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "258f51aa-7081-4562-b3db-952b3c643efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> None\n"
     ]
    }
   ],
   "source": [
    "# In[24]:\n",
    "\n",
    "\n",
    "\"\"\"foreach(f)\n",
    "Returns only those elements which meet the condition of the function inside foreach. In the following example, we call a print function in foreach, which prints all the elements in the RDD.\"\n",
    "\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "def f(x):print(x)\n",
    "fore = words.foreach(f)\n",
    "# coll = words.collect()\n",
    "print(\"Elements in RDD -> %s\" % (fore))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70aae1ba-8171-4c62-bee2-a253ff1a5042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "\"\"\"filter(f)\n",
    "A new RDD is returned containing the elements, which satisfies the function inside the filter.\n",
    "In the following example, we filter out the strings containing ''spark\".\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD -> %s\" % (filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6d61592-05aa-4da9-9701-eba1610b2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "\"\"\"map(f, preservesPartitioning = False)\n",
    "A new RDD is returned by applying a function to each element in the RDD. \n",
    "In the following example, we form a key value pair and map every string with a value of 1.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair -> %s\" % (mapping))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccf48573-b414-487c-a13b-7dd63473a5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "\"\"\"reduce(f)\n",
    "After performing the specified commutative and associative binary operation, the element in the RDD is returned. \n",
    "In the following example,\n",
    "we are importing add package from the operator and applying it on ‘num’ to carry out a simple addition operation.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print(\"Adding all the elements -> %i\" % (adding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20701566-8a23-4cb0-a3bb-fdd250de7ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "\"\"\"join(other, numPartitions = None)\n",
    "It returns RDD with a pair of elements with the matching keys and all the values for that particular key. \n",
    "In the following example, there are two pair of elements in two different RDDs.\n",
    "After joining these two RDDs, we get an RDD with elements having matching keys and their values.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print(\"Join RDD -> %s\" % (final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affac4de-8d81-464e-bc42-cc70ad672649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got chached > True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "\"\"\"cache()\n",
    "Persist this RDD with the default storage level (MEMORY_ONLY). \n",
    "You can also check if the RDD is cached or not.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(\"Words got chached > %s\" % (caching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2747c8e-1076-4287-b13f-add07e8e9af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f658a5-4d01-446b-a1f6-1204ce97ed13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
